{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOP90zRRXImNHe+UdNMNp02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orca3/llm-model-serving/blob/main/ch08/TensorRT_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n"
      ],
      "metadata": {
        "id": "6MsP7wly8okb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get -y install libopenmpi-dev\n"
      ],
      "metadata": {
        "id": "b5azWms_9ycP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade pip setuptools && pip3 install tensorrt_llm"
      ],
      "metadata": {
        "id": "1ttHPsJQ90Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm import LLM, SamplingParams\n"
      ],
      "metadata": {
        "id": "lCdgCpUC94lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model could accept HF model name, a path to local HF model,\n",
        "# or TensorRT Model Optimizer's quantized checkpoints like nvidia/Llama-3.1-8B-Instruct-FP8 on HF.\n",
        "llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# Sample prompts.\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "\n",
        "# Create a sampling params.\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "for output in llm.generate(prompts, sampling_params):\n",
        "    print(\n",
        "        f\"Prompt: {output.prompt!r}, Generated text: {output.outputs[0].text!r}\"\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "XzxL4jJ3-zp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TH7bnWGBAT54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}