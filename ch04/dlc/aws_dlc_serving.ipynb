{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn7Sc2YAYQlhJ8f3aoGvhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orca3/llm-model-serving/blob/main/ch04/dlc/aws_dlc_serving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample code are compiled from AWS tutorial, to show the concept clearly, we trim lots of setup code and only keep the key sudo code in this notebook, for the full executable code and AWS setup instruction, please refer to:\n",
        "\n",
        "\n",
        "* [AWS Large Model Inference Starting Guide](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/starting-guide.html)\n",
        "* [AWS Developer Guide: Deploy models with TorchServe](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-torchserve.html)  \n",
        "* [AWS Developer Guide: Deploy models with DJL Serving](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-djl-serving.html)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hsjP4_8O6RaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example One: Serve Self-built Pytorch Model with AWS TorchServe Image"
      ],
      "metadata": {
        "id": "okFYZwDI70uK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwvRHEtS6NDP"
      },
      "outputs": [],
      "source": [
        "# Find the available serving image by searching model framework\n",
        "# and server instance type\n",
        "baseimage = sagemaker.image_uris.retrieve(\n",
        "        framework=\"pytorch\",\n",
        "        region=\"<region>\",\n",
        "        py_version=\"py310\",\n",
        "        image_scope=\"inference\",\n",
        "        version=\"2.0.1\",\n",
        "        instance_type=\"ml.g4dn.16xlarge\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model file and upload to cloud strage (AWS s3)\n",
        "\n",
        "# package model\n",
        "torch-model-archiver --model-name mnist --version 1.0 --model-file workspace/mnist-dev/mnist.py \\\\\n",
        "  --serialized-file workspace/mnist-dev/mnist_cnn.pt --handler workspace/mnist-dev/mnist_handler.py \\\\\n",
        "  --config-file workspace/mnist-dev/model-config.yaml --archive-format tgz\n",
        "\n",
        "output_path = f\"s3://{bucket_name}/{prefix}/models\"\n",
        "    aws s3 cp mnist.tar.gz {output_path}/mnist.tar.gz\n"
      ],
      "metadata": {
        "id": "GsKI0t6R8Rj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the Model definition include serving image\n",
        "model = Model(model_data = f'{output_path}/mnist.tar.gz',\n",
        "                  image_uri = baseimage,\n",
        "                  predictor_cls = Predictor,\n",
        "                  name = \"mnist\")"
      ],
      "metadata": {
        "id": "oJjVC8AO82_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy model as a model endpoint\n",
        "\n",
        "endpoint_name = 'torchserve-endpoint-1'\n",
        "predictor = model.deploy(\n",
        "    instance_type='ml.g4dn.xlarge',\n",
        "    initial_instance_count=1,\n",
        "    endpoint_name = endpoint_name,\n",
        "    serializer=JSONSerializer(),\n",
        "    deserializer=JSONDeserializer())"
      ],
      "metadata": {
        "id": "Cw_RNqWl9aA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Two: Serve LLM with vLLM on AWS DJL image  "
      ],
      "metadata": {
        "id": "dTet1l7-9ynH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the SageMaker Model object. In this example we let LMI configure the deployment settings based on the model architecture\n",
        "model = DJLModel(\n",
        "  model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "  role=iam_role,\n",
        "  env={\n",
        "    \"HF_TOKEN\": \"<hf token value for gated models>\",\n",
        "    # Add more serving configurations here\n",
        "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"4\", # Example: set tensor parallel degree\n",
        "    \"OPTION_SERVING_LOADER\": \"vllm\", # Example: specify serving loader\n",
        "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"128\", # Example: set max rolling batch size\n",
        "  }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "1JJGF14h9yIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy your model to a SageMaker Endpoint and create a Predictor to make inference requests\n",
        "endpoint_name = sagemaker.utils.name_from_base(\"llama-8b-endpoint\")\n",
        "predictor = model.deploy(\n",
        "    instance_type=\"ml.g5.12xlarge\",\n",
        "    initial_instance_count=1,\n",
        "    endpoint_name=endpoint_name)\n"
      ],
      "metadata": {
        "id": "yJeVVCBu_Gve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}