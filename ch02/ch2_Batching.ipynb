{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORAzHU6Dwlti42EYXjp6Zh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orca3/llm-model-serving/blob/main/ch02/ch2_Batching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvTMMnuG_F7j"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet vllm transformers tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# Unload models and clean up gpu memory cache\n",
        "def free_gpu(model):\n",
        "  if model:\n",
        "    # Removes the reference to the model's memory,\n",
        "    # making it eligible for garbage collection.\n",
        "    del model\n",
        "\n",
        "  # Release any cached GPU memory that's no longer needed.\n",
        "  if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "  # Trigger garbage collection to ensure memory is fully released.\n",
        "  gc.collect()\n",
        "\n",
        "free_gpu(None)"
      ],
      "metadata": {
        "id": "P4PCTvpwG3lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt.\n",
        "prompt = \"\"\"You are an expert AI historian writing a detailed chapter for a book titled \"The Evolution of Human-AI Collaboration.\"\n",
        "\n",
        "Begin by summarizing the early stages of artificial intelligence in the 1950s, touching on symbolic logic and rule-based systems. Then transition into the rise of machine learning, particularly deep learning in the 2010s.\n",
        "\n",
        "Afterward, describe how large language models like GPT transformed human-computer interaction, enabling applications in education, creative writing, customer support, and software development.\n",
        "\n",
        "Finally, reflect on the societal and ethical implications of AI, such as misinformation, bias, and the alignment problem.\n",
        "\n",
        "Write in a formal tone, with rich detail and examples in each era.\"\"\""
      ],
      "metadata": {
        "id": "1zcjOaiaG-5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Load model with vLLM\n",
        "llm = LLM(\n",
        "    model=\"Qwen/Qwen2.5-0.5B\",\n",
        "    dtype=\"float16\",\n",
        "    trust_remote_code=True,\n",
        "    max_model_len=2048\n",
        ")\n"
      ],
      "metadata": {
        "id": "scbrUKg7HCTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import pipeline\n",
        "\n",
        "# Prompts for batch generation, 4 input sequences\n",
        "prompts = [\n",
        "    \"What is the meaning of life?\",\n",
        "    \"Write a short story about a robot learning to love.\",\n",
        "    \"Explain quantum physics in simple terms.\",\n",
        "    \"Translate 'Hello, world!' into Spanish.\"\n",
        "]\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "# process four input sequences together in one batch\n",
        "vllm_outputs = llm.generate(prompts, sampling_params)\n",
        "end_time = time.time()\n",
        "vllm_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nvLLM generation time for 4 prompts in a batch: {vllm_time:.4f} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "# process the first input sequence only\n",
        "vllm_outputs = llm.generate([prompts[0]], sampling_params)\n",
        "end_time = time.time()\n",
        "vllm_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nvLLM generation time for 1 prompts: {vllm_time:.4f} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PZ3FT0uON6U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sG7u6VIcOR_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}