{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKmw1YsVsccfl2ilbqlz4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orca3/llm-model-serving/blob/main/ch02/ch2_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7MInbXv32n97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet vllm transformers tiktoken"
      ],
      "metadata": {
        "id": "1pTMFepDwNZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt.\n",
        "prompt = \"\"\"You are an expert AI historian writing a detailed chapter for a book titled \"The Evolution of Human-AI Collaboration.\"\n",
        "\n",
        "Begin by summarizing the early stages of artificial intelligence in the 1950s, touching on symbolic logic and rule-based systems. Then transition into the rise of machine learning, particularly deep learning in the 2010s.\n",
        "\n",
        "Afterward, describe how large language models like GPT transformed human-computer interaction, enabling applications in education, creative writing, customer support, and software development.\n",
        "\n",
        "Finally, reflect on the societal and ethical implications of AI, such as misinformation, bias, and the alignment problem.\n",
        "\n",
        "Write in a formal tone, with rich detail and examples in each era.\"\"\""
      ],
      "metadata": {
        "id": "yKM1Wmh41Sx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming:\n",
        "1. Return result when generation completes.\n",
        "2. Return as soon as we have a token."
      ],
      "metadata": {
        "id": "4Iucg37wQhNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U8Eh6gl7bee0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from vllm.engine.arg_utils import AsyncEngineArgs\n",
        "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
        "from vllm.sampling_params import SamplingParams\n",
        "\n",
        "# Initialize the engine arguments\n",
        "engine_args = AsyncEngineArgs(\n",
        "    model=\"Qwen/Qwen2.5-0.5B\",\n",
        "    dtype=\"float16\",\n",
        "    tensor_parallel_size=1,      # Number of GPUs to use\n",
        "    gpu_memory_utilization=0.9,  # GPU memory utilization\n",
        "    max_num_batched_tokens=32768, # Maximum number of tokens to process in a batch\n",
        "    max_num_seqs=256,           # Maximum number of sequences to process\n",
        "    disable_log_requests=True,   # Disable request logging\n",
        "    disable_log_stats=True,      # Disable stats logging\n",
        ")\n",
        "\n",
        "# Create the vLLM async streaming engine\n",
        "engine = AsyncLLMEngine.from_engine_args(engine_args)\n"
      ],
      "metadata": {
        "id": "PCNhqxbjQ_FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mention this code is just show, please run streaming.py for actual execution."
      ],
      "metadata": {
        "id": "5RzJ9SwpOVUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sudo code, please run stream.py code to see the execution."
      ],
      "metadata": {
        "id": "Dwmpc7F0RdAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def generate_text(prompt: str, max_tokens: int = 100, request_id=\"id\"):\n",
        "  try:\n",
        "    # Define sampling parameters\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.0,\n",
        "        max_tokens=max_tokens,\n",
        "        stop=[\"\\n\"],  # Stop at newline\n",
        "    )\n",
        "\n",
        "    # Generate text in async and streaming fashion\n",
        "    results_generator = engine.generate(\n",
        "        prompt=prompt,\n",
        "        sampling_params=sampling_params,\n",
        "        request_id=request_id\n",
        "    )\n",
        "\n",
        "    # Process the results\n",
        "    final_output = None\n",
        "    async for request_output in results_generator:\n",
        "        final_output = request_output\n",
        "        # Print each token as it's generated\n",
        "        print(\"chunk \\n\")\n",
        "        for output in request_output.outputs:\n",
        "            print(output.text, end=\"\", flush=True)\n",
        "        print()\n",
        "    print()  # Newline at the end\n",
        "\n",
        "    # This will only be reached if all tokens are generated\n",
        "    print(\"\\nGeneration completed successfully\")\n",
        "\n",
        "    return final_output\n",
        "  except asyncio.CancelledError:\n",
        "    print(\"\\nGeneration was cancelled\")\n",
        "    return None\n",
        "  finally:\n",
        "    # Always clean up\n",
        "    try:\n",
        "      await engine.abort(request_id)\n",
        "    except:\n",
        "      pass\n"
      ],
      "metadata": {
        "id": "gAme1aaoewHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request_id = \"any_id\"\n",
        "await generate_text(prompt, 10000, requtest_id)\n"
      ],
      "metadata": {
        "id": "zi65dpiO-NFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have problem running above code due to Jupyter event loop, try run the example (https://github.com/orca3/llm-model-serving/blob/main/ch02/streaming.py) in terminal."
      ],
      "metadata": {
        "id": "TsO77K61pBsE"
      }
    }
  ]
}