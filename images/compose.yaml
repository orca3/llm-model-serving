services:
  inference:
    image: orca3ai/inference:latest
    ports:
      - "8002:80"
    networks:
      - llm-serving
    volumes:
      - models:/home/serving/models
    environment:
      - PREDICTOR_TYPE="docker"
  torchserve:
    image: pytorch/torchserve:0.1.1-cpu
    ports:
      - "8080:8080"
      - "8081:8081"
      - "8082:8082"
    networks:
      - llm-serving
    volumes:
      - models:/home/model-server/model-store:ro
      - ../predictor/torchserve/config.properties:/home/model-server/config.properties:ro
  llm:
    image: orca3ai/llm-predictor
    ports:
      - "9090:5000"
    networks:
      - llm-serving
    volumes:
      - models:/usr/src/app/models:ro
    environment:
      - MODEL_DIR="./models/llama"
      - HOST_IP="0.0.0.0"
networks:
  llm-serving: {}
volumes:
  models:
    driver: local # Define the driver and options under the volume name
    driver_opts:
      type: none
      device: ../models
      o: bind